<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api//loYzVEEvXwqrHxE6BYmWaj2h+Q</id>
  <title>arXiv Query: search_query=all:Conv-Linformer OR all:Boosting OR all:Linformer OR all:Performance OR all:with OR all:Convolution OR all:in OR all:Small-Scale OR all:Settings&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-23T06:56:39Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:Conv-Linformer+OR+(all:Boosting+OR+(all:Linformer+OR+(all:Performance+OR+(all:with+OR+(all:Convolution+OR+(all:in+OR+(all:Small-Scale+OR+all:Settings)))))))&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>901830</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2308.13875v1</id>
    <title>Performance of Genetic Algorithms in the Context of Software Model Refactoring</title>
    <updated>2023-08-26T13:25:42Z</updated>
    <link href="https://arxiv.org/abs/2308.13875v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2308.13875v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Software systems continuously evolve due to new functionalities, requirements, or maintenance activities. In the context of software evolution, software refactoring has gained a strategic relevance. The space of possible software refactoring is usually very large, as it is given by the combinations of different refactoring actions that can produce software system alternatives. Multi-objective algorithms have shown the ability to discover alternatives by pursuing different objectives simultaneously. Performance of such algorithms in the context of software model refactoring is of paramount importance. Therefore, in this paper, we conduct a performance analysis of three genetic algorithms to compare them in terms of performance and quality of solutions. Our results show that there are significant differences in performance among the algorithms (e.g., PESA2 seems to be the fastest one, while NSGA-II shows the least memory usage).</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-08-26T13:25:42Z</published>
    <arxiv:comment>19th European Workshop on Performance Engineering (EPEW 2023)</arxiv:comment>
    <arxiv:primary_category term="cs.SE"/>
    <arxiv:journal_ref>Computer Performance Engineering and Stochastic Modelling - 19th European Workshop, EPEW 2023</arxiv:journal_ref>
    <author>
      <name>Vittorio Cortellessa</name>
    </author>
    <author>
      <name>Daniele Di Pompeo</name>
    </author>
    <author>
      <name>Michele Tucci</name>
    </author>
    <arxiv:doi>10.1007/978-3-031-43185-2_16</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1007/978-3-031-43185-2_16" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09855v5</id>
    <title>Wide Boosting</title>
    <updated>2022-11-06T03:15:10Z</updated>
    <link href="https://arxiv.org/abs/2007.09855v5" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2007.09855v5" rel="related" type="application/pdf" title="pdf"/>
    <summary>Gradient Boosting (GB) is a popular methodology used to solve prediction problems by minimizing a differentiable loss function, $L$. GB performs very well on tabular machine learning (ML) problems; however, as a pure ML solver it lacks the ability to fit models with probabilistic but correlated multi-dimensional outputs, for example, multiple correlated Bernoulli outputs. GB also does not form intermediate abstract data embeddings, one property of Deep Learning that gives greater flexibility and performance on other types of problems. This paper presents a simple adjustment to GB motivated in part by artificial neural networks. Specifically, our adjustment inserts a matrix multiplication between the output of a GB model and the loss, $L$. This allows the output of a GB model to have increased dimension prior to being fed into the loss and is thus ``wider'' than standard GB implementations. We call our method Wide Boosting (WB) and show that WB outperforms GB on mult-dimesional output tasks and that the embeddings generated by WB contain are more useful in downstream prediction tasks than GB output predictions alone.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-07-20T02:54:50Z</published>
    <arxiv:comment>Gradient Boosting, Wide Neural Networks</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Michael T. Horrell</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13767v1</id>
    <title>Multicalibration as Boosting for Regression</title>
    <updated>2023-01-31T17:05:24Z</updated>
    <link href="https://arxiv.org/abs/2301.13767v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2301.13767v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study the connection between multicalibration and boosting for squared error regression. First we prove a useful characterization of multicalibration in terms of a ``swap regret'' like condition on squared error. Using this characterization, we give an exceedingly simple algorithm that can be analyzed both as a boosting algorithm for regression and as a multicalibration algorithm for a class H that makes use only of a standard squared error regression oracle for H. We give a weak learning assumption on H that ensures convergence to Bayes optimality without the need to make any realizability assumptions -- giving us an agnostic boosting algorithm for regression. We then show that our weak learning assumption on H is both necessary and sufficient for multicalibration with respect to H to imply Bayes optimality. We also show that if H satisfies our weak learning condition relative to another class C then multicalibration with respect to H implies multicalibration with respect to C. Finally we investigate the empirical performance of our algorithm experimentally using an open source implementation that we make available. Our code repository can be found at https://github.com/Declancharrison/Level-Set-Boosting.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-01-31T17:05:24Z</published>
    <arxiv:comment>Code available here: https://github.com/Declancharrison/Level-Set-Boosting</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Ira Globus-Harris</name>
    </author>
    <author>
      <name>Declan Harrison</name>
    </author>
    <author>
      <name>Michael Kearns</name>
    </author>
    <author>
      <name>Aaron Roth</name>
    </author>
    <author>
      <name>Jessica Sorrell</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1149v1</id>
    <title>A Performance Study of GA and LSH in Multiprocessor Job Scheduling</title>
    <updated>2010-02-05T08:39:11Z</updated>
    <link href="https://arxiv.org/abs/1002.1149v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1002.1149v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>  Multiprocessor task scheduling is an important and computationally difficult problem. This paper proposes a comparison study of genetic algorithm and list scheduling algorithm. Both algorithms are naturally parallelizable but have heavy data dependencies. Based on experimental results, this paper presents a detailed analysis of the scalability, advantages and disadvantages of each algorithm. Multiprocessors have emerged as a powerful computing means for running real-time applications, especially where a uni-processor system would not be sufficient enough to execute all the tasks. The high performance and reliability of multiprocessors have made them a powerful computing resource. Such computing environment requires an efficient algorithm to determine when and on which processor a given task should execute. In multiprocessor systems, an efficient scheduling of a parallel program onto the processors that minimizes the entire execution time is vital for achieving a high performance. This scheduling problem is known to be NP- Hard. In multiprocessor scheduling problem, a given program is to be scheduled in a given multiprocessor system such that the program's execution time is minimized. The last job must be completed as early as possible. Genetic algorithm (GA) is one of the widely used techniques for constrained optimization.</summary>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <published>2010-02-05T08:39:11Z</published>
    <arxiv:comment>International Journal of Computer Science Issues, IJCSI, Vol. 7, Issue 1, No. 1, January 2010, http://ijcsi.org/articles/A-Performance-Study-of-GA-and-LSH-in-Multiprocessor-Job-Scheduling.php</arxiv:comment>
    <arxiv:primary_category term="cs.PF"/>
    <arxiv:journal_ref>International Journal of Computer Science Issues, IJCSI, Vol. 7, Issue 1, No. 1, January 2010, http://ijcsi.org/articles/A-Performance-Study-of-GA-and-LSH-in-Multiprocessor-Job-Scheduling.php</arxiv:journal_ref>
    <author>
      <name>S. R. Vijayalakshmi</name>
    </author>
    <author>
      <name>G. Padmavathi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1154v1</id>
    <title>Performance Analysis of Software to Hardware Task Migration in Codesign</title>
    <updated>2010-02-05T08:51:44Z</updated>
    <link href="https://arxiv.org/abs/1002.1154v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1002.1154v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>  The complexity of multimedia applications in terms of intensity of computation and heterogeneity of treated data led the designers to embark them on multiprocessor systems on chip. The complexity of these systems on one hand and the expectations of the consumers on the other hand complicate the designers job to conceive and supply strong and successful systems in the shortest deadlines. They have to explore the different solutions of the design space and estimate their performances in order to deduce the solution that respects their design constraints. In this context, we propose the modeling of one of the design space possible solutions: the software to hardware task migration. This modeling exploits the synchronous dataflow graphs to take into account the different migration impacts and estimate their performances in terms of throughput.</summary>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <published>2010-02-05T08:51:44Z</published>
    <arxiv:comment>International Journal of Computer Science Issues, IJCSI, Vol. 7, Issue 1, No. 1, January 2010, http://ijcsi.org/articles/Performance-Analysis-of-Software-to-Hardware-Task-Migration-in-Codesign.php</arxiv:comment>
    <arxiv:primary_category term="cs.PF"/>
    <arxiv:journal_ref>International Journal of Computer Science Issues, IJCSI, Vol. 7, Issue 1, No. 1, January 2010, http://ijcsi.org/articles/Performance-Analysis-of-Software-to-Hardware-Task-Migration-in-Codesign.php</arxiv:journal_ref>
    <author>
      <name>Dorsaf Sebai</name>
    </author>
    <author>
      <name>Abderrazak Jemai</name>
    </author>
    <author>
      <name>Imed Bennour</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08294v2</id>
    <title>Coherence boosting: When your pretrained language model is not paying enough attention</title>
    <updated>2022-03-16T15:49:26Z</updated>
    <link href="https://arxiv.org/abs/2110.08294v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2110.08294v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Long-range semantic coherence remains a challenge in automatic language generation and understanding. We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction. We present coherence boosting, an inference procedure that increases a LM's focus on a long context. We show the benefits of coherence boosting with pretrained models by distributional analyses of generated ordinary text and dialog responses. It is also found that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-10-15T18:05:33Z</published>
    <arxiv:comment>ACL 2022; code: https://github.com/zhenwang9102/coherence-boosting</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Nikolay Malkin</name>
    </author>
    <author>
      <name>Zhen Wang</name>
    </author>
    <author>
      <name>Nebojsa Jojic</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07996v1</id>
    <title>Novel Convolution Kernels for Computer Vision and Shape Analysis based on Electromagnetism</title>
    <updated>2018-06-20T21:31:00Z</updated>
    <link href="https://arxiv.org/abs/1806.07996v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1806.07996v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Computer vision is a growing field with a lot of new applications in automation and robotics, since it allows the analysis of images and shapes for the generation of numerical or analytical information. One of the most used method of information extraction is image filtering through convolution kernels, with each kernel specialized for specific applications. The objective of this paper is to present a novel convolution kernels, based on principles of electromagnetic potentials and fields, for a general use in computer vision and to demonstrate its usage for shape and stroke analysis. Such filtering possesses unique geometrical properties that can be interpreted using well understood physics theorems. Therefore, this paper focuses on the development of the electromagnetic kernels and on their application on images for shape and stroke analysis. It also presents several interesting features of electromagnetic kernels, such as resolution, size and orientation independence, robustness to noise and deformation, long distance stroke interaction and ability to work with 3D images</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-06-20T21:31:00Z</published>
    <arxiv:comment>Keywords: Shape analysis; Stroke analysis; Computer vision; Electromagnetic potential field; Feature extraction; Image filtering; Image convolution Published in PolyPublie: https://publications.polymtl.ca/3162/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <arxiv:journal_ref>Beaini, D., Achiche, S., Law-Kam Cio, Y.-S. &amp; Raison, M. (2018). Novel convolution kernels for computer vision and shape analysis based on electromagnetism (Report). https://publications.polymtl.ca/3162/</arxiv:journal_ref>
    <author>
      <name>Dominique Beaini</name>
    </author>
    <author>
      <name>Sofiane Achiche</name>
    </author>
    <author>
      <name>Yann-Seing Law-Kam Cio</name>
    </author>
    <author>
      <name>Maxime Raison</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07409v1</id>
    <title>A Metric for Performance Portability</title>
    <updated>2016-11-22T16:50:42Z</updated>
    <link href="https://arxiv.org/abs/1611.07409v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1611.07409v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The term "performance portability" has been informally used in computing to refer to a variety of notions which generally include: 1) the ability to run one application across multiple hardware platforms; and 2) achieving some notional level of performance on these platforms. However, there has been a noticeable lack of consensus on the precise meaning of the term, and authors' conclusions regarding their success (or failure) to achieve performance portability have thus been subjective. Comparing one approach to performance portability with another has generally been marked with vague claims and verbose, qualitative explanation of the comparison. This paper presents a concise definition for performance portability, along with a simple metric that accurately captures the performance and portability of an application across different platforms. The utility of this metric is then demonstrated with a retroactive application to previous work.</summary>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <published>2016-11-22T16:50:42Z</published>
    <arxiv:comment>7 pages, in Proceedings of the 7th International Workshop in Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems</arxiv:comment>
    <arxiv:primary_category term="cs.PF"/>
    <author>
      <name>S. J. Pennycook</name>
    </author>
    <author>
      <name>J. D. Sewall</name>
    </author>
    <author>
      <name>V. W. Lee</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/astro-ph/0412305v1</id>
    <title>Cosmological Origin of Small-Scale Clumps and DM Annihilation Signal</title>
    <updated>2004-12-13T17:23:12Z</updated>
    <link href="https://arxiv.org/abs/astro-ph/0412305v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/astro-ph/0412305v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>  We study the cosmological origin of small-scale DM clumps in the hierarchical scenario with the most conservative assumption of adiabatic Gaussian fluctuations. The mass spectrum of small-scale clumps with M&lt;10^3Msun is calculated with tidal destruction of the clumps taken into account within the hierarchical model of clump structure. Only 0.1-0.5% of small clumps survive the stage of tidal destruction in each logarithmic mass interval. The mass distribution of clumps has a cutoff at Mmin due to diffusion of DM particles out of a fluctuation and free streaming at later stage. Mmin is a model dependent quantity. In the case the neutralino DM, considered as a pure bino, Mmin~10^-8 Msun. The evolution of density profile in a DM clump does not result in the singularity because of formation of the core under influence of tidal interaction. The radius of the core is ~0.1R, where R is radius of the clump. The applications for annihilation of DM particles in the Galactic halo are studied. The number density of clumps as a function of their mass, radius and distance to the Galactic center is presented. The enhancement of annihilation signal due to clumpiness, valid for arbitrary DM particles, is calculated. In spite of small survival probability, the global annihilation signal in most cases is dominated by clumps, with major contribution given by small clumps. The enhancement due to large clumps with M&gt;10^6 Msun is very small.</summary>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2004-12-13T17:23:12Z</published>
    <arxiv:comment>The new element of this paper is estimation of contribution to annihilation from large-scale clumps in comparison with small ones</arxiv:comment>
    <arxiv:primary_category term="astro-ph"/>
    <arxiv:journal_ref>Proceedings of the 6th RESCEU International Symposium on "Frontier in Astroparticle Physics and Cosmology", eds. K. Sato and S. Nagataki (Universal Academy Press Inc.: Tokyo, Japan, 2004), p241-248</arxiv:journal_ref>
    <author>
      <name>Veniamin Berezinsky</name>
    </author>
    <author>
      <name>Vyacheslav Dokuchaev</name>
    </author>
    <author>
      <name>Yury Eroshenko</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2266v1</id>
    <title>Enhanced Cluster Computing Performance Through Proportional Fairness</title>
    <updated>2014-04-08T09:44:11Z</updated>
    <link href="https://arxiv.org/abs/1404.2266v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1404.2266v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The performance of cluster computing depends on how concurrent jobs share multiple data center resource types like CPU, RAM and disk storage. Recent research has discussed efficiency and fairness requirements and identified a number of desirable scheduling objectives including so-called dominant resource fairness (DRF). We argue here that proportional fairness (PF), long recognized as a desirable objective in sharing network bandwidth between ongoing flows, is preferable to DRF. The superiority of PF is manifest under the realistic modelling assumption that the population of jobs in progress is a stochastic process. In random traffic the strategy-proof property of DRF proves unimportant while PF is shown by analysis and simulation to offer a significantly better efficiency-fairness tradeoff.</summary>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-04-08T09:44:11Z</published>
    <arxiv:comment>Submitted to Performance 2014</arxiv:comment>
    <arxiv:primary_category term="cs.PF"/>
    <author>
      <name>Thomas Bonald</name>
    </author>
    <author>
      <name>James Roberts</name>
    </author>
  </entry>
</feed>
