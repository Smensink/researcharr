<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/PiI8P1P+mxg7/2LRTE+NwVZBOHQ</id>
  <title>arXiv Query: search_query=all:"Conv-Linformer"&amp;id_list=&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-23T06:57:31Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:Conv-Linformer&amp;start=0&amp;max_results=10&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>1</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2202.02884v2</id>
    <title>Exploring Self-Attention Mechanisms for Speech Separation</title>
    <updated>2023-05-27T17:44:21Z</updated>
    <link href="https://arxiv.org/abs/2202.02884v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2202.02884v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Transformers have enabled impressive improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we proposed the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets. This paper studies in-depth Transformers for speech separation. In particular, we extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets, such as LibriMix, WHAM!, and WHAMR!. Moreover, we extend our model to perform speech enhancement and provide experimental evidence on denoising and dereverberation tasks. Finally, we investigate, for the first time in speech separation, the use of efficient self-attention mechanisms such as Linformers, Lonformers, and ReFormers. We found that they reduce memory requirements significantly. For example, we show that the Reformer-based attention outperforms the popular Conv-TasNet model on the WSJ0-2Mix dataset while being faster at inference and comparable in terms of memory consumption.</summary>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-02-06T23:13:27Z</published>
    <arxiv:comment>Accepted to IEEE/ACM Transactions on Audio, Speech, and Language Processing</arxiv:comment>
    <arxiv:primary_category term="eess.AS"/>
    <author>
      <name>Cem Subakan</name>
    </author>
    <author>
      <name>Mirco Ravanelli</name>
    </author>
    <author>
      <name>Samuele Cornell</name>
    </author>
    <author>
      <name>Francois Grondin</name>
    </author>
    <author>
      <name>Mirko Bronzi</name>
    </author>
  </entry>
</feed>
